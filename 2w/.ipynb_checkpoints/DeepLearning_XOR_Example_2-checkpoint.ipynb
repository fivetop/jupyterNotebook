{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVPjIORrOxp8"
   },
   "source": [
    "## numerical_derivative, sigmoid 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ONLApAPKOxqO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = float(tmp_val) - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l9KYhcJ6Oxqb"
   },
   "outputs": [],
   "source": [
    "# 최종출력은 y = sigmoid(Wx+b) 이며, 손실함수는 cross-entropy 로 나타냄\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLkHhqE1PB_C"
   },
   "source": [
    "## data definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dR6A0Sc1Oxqc",
    "outputId": "fbd0f5ee-e7f1-4246-b662-93523624301f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xor_xdata.shape =  (4, 2) , xor_tdata.shape =  (4, 1)\n"
     ]
    }
   ],
   "source": [
    "xor_xdata = np.array([ [0,0], [0,1], [1,0], [1,1] ])  \n",
    "xor_tdata = np.array([0, 1, 1, 0]).reshape(4,1)\n",
    "\n",
    "print(\"xor_xdata.shape = \", xor_xdata.shape, \", xor_tdata.shape = \", xor_tdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFnhA47FPFzp"
   },
   "source": [
    "## initialize weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MtmDwl2Oxqf",
    "outputId": "afde59a9-baaa-453f-df93-5b869d48d620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5488135  0.71518937]\n",
      " [0.60276338 0.54488318]] [0.92559664 0.07103606]\n",
      "[[0.4236548  0.64589411 0.43758721]\n",
      " [0.891773   0.96366276 0.38344152]] [0.0871293  0.0202184  0.83261985]\n",
      "[[0.79172504]\n",
      " [0.52889492]\n",
      " [0.56804456]] [0.77815675]\n"
     ]
    }
   ],
   "source": [
    "input_nodes = 2\n",
    "hidden_nodes_1 = 2\n",
    "hidden_nodes_2 = 3\n",
    "output_nodes = 1\n",
    "\n",
    "W2 = np.random.rand(input_nodes, hidden_nodes_1)  \n",
    "W3 = np.random.rand(hidden_nodes_1, hidden_nodes_2)  \n",
    "W4 = np.random.rand(hidden_nodes_2, output_nodes)  \n",
    "\n",
    "b2 = np.random.rand(hidden_nodes_1)   \n",
    "b3 = np.random.rand(hidden_nodes_2)\n",
    "b4 = np.random.rand(output_nodes)\n",
    "\n",
    "print(W2, b2)\n",
    "print(W3, b3)\n",
    "print(W4, b4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIrUcRvcPQeG"
   },
   "source": [
    "## define loss function and output, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ldAtOucnOxqg"
   },
   "outputs": [],
   "source": [
    "def loss_func(x, t):\n",
    "    \n",
    "    delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "    z2 = np.dot(x, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    z3 = np.dot(a2, W3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    z4 = np.dot(a3, W4) + b4\n",
    "    y = a4 = sigmoid(z4)\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( t*np.log(y + delta) + (1-t)*np.log((1 - y)+delta ) ) \n",
    "\n",
    "    # MSE\n",
    "    #return np.sum((t-y)**2) / len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OAKzLBvPU5e"
   },
   "source": [
    "## XOR learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uaeY4ijOxqi",
    "outputId": "0ca50e18-2c47-494b-9236-8e572374daf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value =  4.849305410095795\n",
      "step =  0 loss value =  4.1777115118820705\n",
      "step =  500 loss value =  2.770941283666491\n",
      "step =  1000 loss value =  2.7664802420290338\n",
      "step =  1500 loss value =  2.7413533688564256\n",
      "step =  2000 loss value =  2.3646538550098035\n",
      "step =  2500 loss value =  1.7515501482281102\n",
      "step =  3000 loss value =  0.07508785553454016\n",
      "step =  3500 loss value =  0.02897916600148879\n",
      "step =  4000 loss value =  0.01732945166866684\n",
      "step =  4500 loss value =  0.0121942045507772\n",
      "step =  5000 loss value =  0.009341273402909286\n",
      "step =  5500 loss value =  0.007538553967937851\n",
      "step =  6000 loss value =  0.006301679173425055\n",
      "step =  6500 loss value =  0.005403004252331794\n",
      "step =  7000 loss value =  0.004721922866118095\n",
      "step =  7500 loss value =  0.00418877132590911\n",
      "step =  8000 loss value =  0.0037605897737936662\n",
      "step =  8500 loss value =  0.0034094897494881186\n",
      "step =  9000 loss value =  0.0031166011009082107\n",
      "step =  9500 loss value =  0.002868712383417655\n",
      "step =  10000 loss value =  0.0026563053537430785\n",
      "step =  10500 loss value =  0.0024723534778436224\n",
      "step =  11000 loss value =  0.002311559667616807\n",
      "step =  11500 loss value =  0.0021698570101552866\n",
      "step =  12000 loss value =  0.0020440725783927266\n",
      "step =  12500 loss value =  0.0019316954984579227\n",
      "step =  13000 loss value =  0.0018307134770768128\n",
      "step =  13500 loss value =  0.001739495362699656\n",
      "step =  14000 loss value =  0.0016567053228607022\n",
      "step =  14500 loss value =  0.001581239152613634\n",
      "step =  15000 loss value =  0.001512176342743335\n",
      "step =  15500 loss value =  0.0014487435469067484\n",
      "step =  16000 loss value =  0.0013902864116433283\n",
      "step =  16500 loss value =  0.0013362476219685219\n",
      "step =  17000 loss value =  0.0012861496223630653\n",
      "step =  17500 loss value =  0.00123958089350108\n",
      "step =  18000 loss value =  0.0011961849609654516\n",
      "step =  18500 loss value =  0.0011556515229726111\n",
      "step =  19000 loss value =  0.0011177092362364344\n",
      "step =  19500 loss value =  0.001082119810041845\n",
      "step =  20000 loss value =  0.00104867314035061\n",
      "step =  20500 loss value =  0.0010171832767790858\n",
      "step =  21000 loss value =  0.0009874850610713944\n",
      "step =  21500 loss value =  0.000959431310340686\n",
      "step =  22000 loss value =  0.0009328904450234539\n",
      "step =  22500 loss value =  0.0009077444818581371\n",
      "step =  23000 loss value =  0.0008838873281248696\n",
      "step =  23500 loss value =  0.0008612233257648498\n",
      "step =  24000 loss value =  0.0008396660037782672\n",
      "step =  24500 loss value =  0.0008191370049974317\n",
      "step =  25000 loss value =  0.0007995651595128045\n",
      "step =  25500 loss value =  0.0007808856819355347\n",
      "step =  26000 loss value =  0.0007630394736195654\n",
      "step =  26500 loss value =  0.0007459725142400829\n",
      "step =  27000 loss value =  0.0007296353296450277\n",
      "step =  27500 loss value =  0.000713982525097561\n",
      "step =  28000 loss value =  0.000698972374735728\n",
      "step =  28500 loss value =  0.0006845664594927404\n",
      "step =  29000 loss value =  0.0006707293469982258\n",
      "step =  29500 loss value =  0.000657428307834952\n",
      "step =  30000 loss value =  0.0006446330634919242\n",
      "\n",
      "Elapsed Time =>  0:01:46.708005\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1  # 1e-2 에서 변경한 부분\n",
    "\n",
    "f = lambda x : loss_func(xor_xdata, xor_tdata)  \n",
    "\n",
    "print(\"Initial loss value = \", loss_func(xor_xdata, xor_tdata) )\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in range(30001):  \n",
    "    \n",
    "    W2 -= learning_rate * numerical_derivative(f, W2)\n",
    "    \n",
    "    b2 -= learning_rate * numerical_derivative(f, b2)\n",
    "\n",
    "    W3 -= learning_rate * numerical_derivative(f, W3)\n",
    "    \n",
    "    b3 -= learning_rate * numerical_derivative(f, b3)\n",
    "    \n",
    "    W4 -= learning_rate * numerical_derivative(f, W4)\n",
    "    \n",
    "    b4 -= learning_rate * numerical_derivative(f, b4)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"step = \", step, \"loss value = \", loss_func(xor_xdata, xor_tdata) )\n",
    "        \n",
    "end_time = datetime.now()\n",
    "        \n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdCQM7SiPL_N"
   },
   "source": [
    "## evaluate and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Eb-JN1wnPOkt"
   },
   "outputs": [],
   "source": [
    "# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
    "# 입력변수 test_data : numpy type\n",
    "def predict(test_data):\n",
    "    \n",
    "    z2 = np.dot(test_data, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    z3 = np.dot(a2, W3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    z4 = np.dot(a3, W4) + b4\n",
    "    y = a4 = sigmoid(z4)\n",
    "    \n",
    "    if y > 0.5:\n",
    "        pred_val = 1\n",
    "    else:\n",
    "        pred_val = 0\n",
    "\n",
    "    return y, pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kypteKwcOxqk",
    "outputId": "36e41ed7-3c1f-4eb5-b4dc-0bf5ad3c4711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.00019022]), 0)\n",
      "(array([0.99986065]), 1)\n",
      "(array([0.99986062]), 1)\n",
      "(array([0.00017602]), 0)\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "\n",
    "    print(predict(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBEvqwwhOxqt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning_XOR_Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
